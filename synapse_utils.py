import os
import subprocess
import shutil
import tempfile
import pandas as pd
import csv
import pickle
import synapse_plot_utils as sp
from deriva.core import HatracStore, ErmrestCatalog, ErmrestSnapshot, get_credential, DerivaPathError
from bdbag import bdbag_api as bdb
from synspy.analyze.pair import SynapticPairStudy, ImageGrossAlignment, transform_points

synapseserver = 'synapse.isrd.isi.edu'

# Configuring the logger for debug level will display the uri's generated by the api
debug = False
if debug:
    import logging
    logger = logging.getLogger('deriva_common.datasets')
    logger.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    logger.addHandler(ch)


# Return the git revision as a string
def git_version():
    def _minimal_ext_cmd(cmd):
        # construct minimal environment
        env = {}
        for k in ['SYSTEMROOT', 'PATH']:
            v = os.environ.get(k)
            if v is not None:
                env[k] = v
        # LANGUAGE is used on win32
        env['LANGUAGE'] = 'C'
        env['LANG'] = 'C'
        env['LC_ALL'] = 'C'
        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]
        return out

    try:
        out = _minimal_ext_cmd(['git', 'rev-parse', 'HEAD'])
        GIT_REVISION = out.strip().decode('ascii')
    except OSError:
        GIT_REVISION = "Unknown"

    return GIT_REVISION


def group_studies(studies, group='Type'):
    """
    Return a dictionary whose key is a type, subject, or alignment and whose value is a list of studies
    """
    if group == 'Type':
        key = 'Type'
    if group == 'Subject':
        key = 'Subject'
    if group == 'Aligned':
        key = 'Aligned'
    g = dict()
    for i in studies:
        g[i[key]] = g.get(i[key], []) + [i]
    return g


def get_synapse_studies(catalog, studyset):
    """
    Get the current list of synapse studys.
    :param protocols:
    :return:
    """

    pb = catalog.getPathBuilder()

    # convenient name for the schema we care about.
    zebrafish = pb.Zebrafish
    synapse = pb.Synapse

    # Lets get some shortcuts for awkward table names.
    cohort_table = zebrafish.tables['Cohort Analysis']
    study_table = zebrafish.tables['Synaptic Pair Study']
    pair_table = zebrafish.tables['Image Pair Study']

    # Build up the path cohort->study->pair->behavior.
    # Make aliases for study and pair instances for later use.
    # We use the left join to make sure we get controls, which will be studies without behaviors.
    path = cohort_table.alias('studyset')\
        .link(zebrafish.tables['Cohort Analysis_Synaptic Pair Study']) \
        .link(study_table.alias('study')) \
        .link(pair_table.alias('pair')) \
        .link(zebrafish.Behavior, join_type='left', on=(pair_table.Subject == zebrafish.Behavior.Subject))

    # Now lets go back an pick up the protocols which are associated with an image.
    # Each image has a protocol step, and from the step we can get the protocol.
    # Use the first image.
    path = path.pair.link(zebrafish.Image.alias('image'), on=path.pair.column_definitions['Image 1'] == zebrafish.Image.ID) \
        .link(synapse.tables['Protocol Step']) \
        .link(synapse.Protocol)

    # Now just pick out the studyset we want.
    path = path.filter(path.studyset.RID == studyset)

    # Now that we have build up the path, we can retrieve the set of studies and associated values
    study_entities = path.study.entities(Study=path.study.ID,
                                         Subject=path.pair.Subject,
                                         Region1=path.study.column_definitions['Synaptic Region 1'],
                                         Region2=path.study.column_definitions['Synaptic Region 2'],
                                         BeforeURL=path.study.column_definitions['Region 1 URL'],
                                         AfterURL=path.study.column_definitions['Region 2 URL'],
                                         BeforeImageID=path.pair.column_definitions['Image 1'],
                                         AfterImageID=path.pair.column_definitions['Image 2'],
                                         Learner=path.Behavior.column_definitions['Learned?'],
                                         AlignP0=path.image.column_definitions['Align P0 ZYX'],
                                         AlignP1=path.image.column_definitions['Align P1 ZYX'],
                                         AlignP2=path.image.column_definitions['Align P2 ZYX'],
                                         Protocol=path.Protocol.ID )

    return study_entities


def get_studies(studyid):
    credential = get_credential(synapseserver)
    if '@' in studyid:
        ermrest_catalog = ErmrestSnapshot('https', synapseserver, 1, credential)
    else:
        ermrest_catalog = ErmrestCatalog('https', synapseserver, 1, credential).latest_snapshot()

    githash = git_version()
    ermrest_snapshot = ermrest_catalog.snaptime

# Get the current list of studies from the server.
    study_entities = get_synapse_studies(ermrest_catalog,studyid)

    print('Identified %d studies' % len(study_entities))

    protocol_types = {
        'PrcDsy20160101A': 'aversion',
        'PrcDsy20170613A': 'conditioned-control',
        'PrcDsy20170615A': 'unconditioned-control',
        'PrcDsy20170613B': 'fullcycle-control',
        'PrcDsy20171030A': 'groundtruth-control',
        'PrcDsy20171030B': 'interval-groundtruth-control'
    }

    # Compute the alignment for each study, and fill in some useful values.
    for i in study_entities:
        i['Paired'] = False
        if protocol_types[i['Protocol']] == 'aversion':
            if i['Learner'] is True:
                i['Type'] = 'learner'
            else:
                i['Type'] = 'nonlearner'
        else:
            i['Type'] = protocol_types[i['Protocol']]

        try:
            i['Aligned'] = False
            i['Provenence'] = {'GITHash': githash, 'CatlogVersion': ermrest_snapshot}
            i['StudyID'] = studyid
            i['Alignment'] = ImageGrossAlignment.from_image_id(ermrest_catalog, i['BeforeImageID'])
            p = pd.DataFrame([i[pt] for pt in ['AlignP0', 'AlignP1', 'AlignP2']])
            p = p.multiply(pd.DataFrame([{'z': 0.4, 'y': 0.26, 'x': 0.26}]*3))

            i['StudyAlignmentPts'] = pd.DataFrame(transform_points(i['Alignment'].M_canonical, p.loc[:,['x','y','z']]),
                                                  columns=['x', 'y', 'z'])
#            i['StudyAlignmentPts'] = pd.DataFrame(transform_points(i['Alignment'].M, p.loc[:,['x','y','z']]),
#                                                columns=['x', 'y', 'z'])
            i['Aligned'] = True
            i['AlignmentPts'] = dict()
        except ValueError:  # Alignments missing....
            print('Alingment missing for study: {0}'.format(i['Study']))
            continue
        except NotImplementedError:
            print('Alignment Code Failed for study: {0}'.format(i['Study']))
            continue

    return {'StudyID': studyid,
            'Studies': list(study_entities),
            'Provenence': {'GITHash': githash, 'CatlogVersion': ermrest_snapshot}
            }


def get_synapses(study):
    """
     synapsefile: a HatracStore object
     study: a dictionary that has URLs for the two images, before and after

     returns two pandas that have the synapses in them.
     """
    credential = get_credential(synapseserver)
    objectstore = HatracStore('https', synapseserver, credentials=credential)

    # Get a path for a tempory file to store HATRAC results
    path = os.path.join(tempfile.mkdtemp(), 'image')
    try:
        # Get the before image from hatrac, be careful in case its missing
        if study['BeforeURL']:
            objectstore.get_obj(study['BeforeURL'], destfilename=path)
            img1 = pd.read_csv(path)
            if True:
                img1.drop(img1.index[0], inplace=True)
        else:
            img1 = None

        # Get the after image from hatrac, be careful in case its missing
        if study['AfterURL']:
            objectstore.get_obj(study['AfterURL'], destfilename=path)
            img2 = pd.read_csv(path)
            if True:
                img2.drop(img2.index[0], inplace=True)
        else:
            img2 = None
    finally:
        shutil.rmtree(os.path.dirname(path))
    return {'Before': img1, 'After': img2, 'Type': study['Type'], 'Study': study['Study'], 'Subject': study['Subject']}


def copy_synapse_files(objectstore, study):
    """
    Copy the files associated with a study into a local directory
    """

    for URL in [study['BeforeURL'], study['AfterURL']]:
        # Get a path for a tempory file to store HATRAC results
        tmpfile = os.path.join(tempfile.mkdtemp(), 'image')
        try:

            # Create an output directory for synapse files.
            os.makedirs('synapse-data', mode=0o777, exist_ok=True)

            # Get the before image from hatrac, be careful in case its missing
            if URL:
                objectstore.get_obj(URL, destfilename=tmpfile)

                # Get the file name from the URL
                hatracfilename = (os.path.basename(URL.split(':')[0]))

                # Now copy the file from the tmp directory to where it will end up....
                with open(tmpfile) as synapse:
                    with open('synapse-data/' + hatracfilename, 'w') as outfile:
                        # Write header
                        outfile.write(synapse.readline())

                        # Skip second line
                        f = synapse.readline()
                        if 'saved,parameters' not in f:
                            outfile.write(f)

                        # now copy the rest of the file...
                        for f in synapse:
                            outfile.write(f)
        finally:
            shutil.rmtree(os.path.dirname(tmpfile))


def studyset_to_bag(studyset, dest, protocol_types, bag_metadata=None, publish=False):
    """
    Export all of the synapse data for every study in the study list.
    Also output a CVS file that contains an index of all of the data.

    The data indes is: StudyID, SubjectID, Study Type, FileNames for Before and After synapses.

    """

    bag_metadata = bag_metadata if bag_metadata else {}

    study_list = studyset['Studies']

    current_dir = os.getcwd()
    try:
        os.chdir(dest)

        # Create an output directory for synapse files.
        os.makedirs('synapse-studies', mode=0o777, exist_ok=True)
        os.chdir('synapse-studies')

        dumpdir = os.getcwd()

        os.makedirs('synapse-data', mode=0o777, exist_ok=True)

        for study in study_list:
            # radius of four....
            for d in ['UnpairedBefore','UnpairedAfter','PairedBefore']:
                fn = 'synapse-data/' + study['Study'] + '-' + study['Type'] + '-' + d + '.csv'
                study[d][4]['Data'].to_csv(fn)

        # Now write out the CSV file will the list of studies...
        with open('studies.csv', 'w', newline='') as csvfile:
            synapsewriter = csv.writer(csvfile)

            # Write out header....
            synapsewriter.writerow(['Study', 'Subject', 'Type'])
            for study in study_list:
                url1 = study['BeforeURL']
                url2 = study['AfterURL']

                filename1 = filename2 = ''
                if url1:
                    filename1 = (os.path.basename(url1.split(':')[0]))
                if url2:
                    filename2 = (os.path.basename(url2.split(':')[0]))

                synapsewriter.writerow([study['Study'], study['Subject'], study['Type']])

        bdb.make_bag(dumpdir, metadata=bag_metadata)
        archivefile = bdb.archive_bag(dumpdir, 'zip')

        if publish:
            bagstore = HatracStore('https', 'synapse-dev.isrd.isi.edu', credentials=credential)
            hatrac_path = '/hatrac/Data/synapse-{0}'.format(bag_metadata['ERMRest-Snapshot'])
            return bagstore.put_obj(hatrac_path, archivefile)
    finally:
        os.chdir(current_dir)
    return archivefile


def synapses_to_bag(study_list, dest, protocol_types, bag_metadata=None, publish=False):
    """
    Export all of the synapse data for every study in the study list.
    Also output a CVS file that contains an index of all of the data.

    The data indes is: StudyID, SubjectID, Study Type, FileNames for Before and After synapses.

    """

    bag_metadata = bag_metadata if bag_metadata else {}

    credential = get_credential("synapse.isrd.isi.edu")
    objectstore = HatracStore('https', 'synapse.isrd.isi.edu', credentials=credential)

    current_dir = os.getcwd()
    try:
        os.chdir(dest)

        # Create an output directory for synapse files.
        os.makedirs('synapse-studies', mode=0o777, exist_ok=True)
        os.chdir('synapse-studies')
        dumpdir = os.getcwd()

        for study in study_list:
            copy_synapse_files(objectstore, study)

        # Now write out the CSV file will the list of studies...
        with open('studies.csv', 'w', newline='') as csvfile:
            synapsewriter = csv.writer(csvfile)

            # Write out header....
            synapsewriter.writerow(['Study', 'Subject', 'Type', 'Learner', 'Before', 'After'])
            for study in study_list:
                study_type = protocol_types[study['Protocol']]
                url1 = study['BeforeURL']
                url2 = study['AfterURL']

                filename1 = filename2 = ''
                if url1:
                    filename1 = (os.path.basename(url1.split(':')[0]))
                if url2:
                    filename2 = (os.path.basename(url2.split(':')[0]))

                synapsewriter.writerow([study['Study'], study['Subject'], study_type, study['Learner'],
                                        filename1, filename2])

        bdb.make_bag(dumpdir, metadata=bag_metadata)
        archivefile = bdb.archive_bag(dumpdir, 'zip')

        if publish:
            bagstore = HatracStore('https', 'synapse-dev.isrd.isi.edu', credentials=credential)
            hatrac_path = '/hatrac/Data/synapse-{0}'.format(bag_metadata['ERMRest-Snapshot'])
            return bagstore.put_obj(hatrac_path, archivefile)
    finally:
        os.chdir(current_dir)
    return archivefile


def compute_pairs(studylist, radii, ratio=None, maxratio=None):
    print('Finding pairs for {0} studies'.format(len(studylist)))

    credential = get_credential(synapseserver)
    ermrest_catalog = ErmrestCatalog('https', synapseserver, 1, credential)
    hatrac_store = HatracStore('https', synapseserver, credentials=credential)

    pairlist = []
    for s in studylist:
        syn_study_id = s['Study']
        s['Paired'] = True

        print('Processing study {0}'.format(syn_study_id))
        study = SynapticPairStudy.from_study_id(ermrest_catalog, syn_study_id)
        try:
            study.retrieve_data(hatrac_store)
        except DerivaPathError:
            print('Study {0} missing synaptic pair'.format(syn_study_id))
            continue
        pairlist.append(s)

        # Compute the actual pairs for the given distances
        s1_to_s2, s2_to_s1 = study.syn_pairing_maps(radii, ratio, maxratio)

        # get results for different radii, store them in a dictonary of pandas
        for i, r in enumerate(radii):

            unpaired1 = study.get_unpaired(s1_to_s2[i, :], study.s1)
            unpaired2 = study.get_unpaired(s2_to_s1[i, :], study.s2)
            paired1, paired2 = study.get_pairs(s1_to_s2[i, :], study.s1, study.s2)

            p = pd.DataFrame(unpaired1[:, 0:5], columns=['z', 'y', 'x', 'core', 'hollow'])
            s['UnpairedBefore'] = s.get('UnpairedBefore', dict())
            s['UnpairedBefore'][r] = {'Data': p}

            p = pd.DataFrame(unpaired2[:, 0:5], columns=['z', 'y', 'x', 'core', 'hollow'])
            s['UnpairedAfter'] = s.get('UnpairedAfter', dict())
            s['UnpairedAfter'][r] = {'Data': p}

            p = pd.DataFrame(paired1[:, 0:5], columns=['z', 'y', 'x', 'core', 'hollow'])
            s['PairedBefore'] = s.get('PairedBefore', dict())
            s['PairedBefore'][r] = {'Data': p}

            p = pd.DataFrame(paired2[:, 0:5], columns=['z', 'y', 'x', 'core', 'hollow'])
            s['PairedAfter'] = s.get('PairedAfter', dict())
            s['PairedAfter'][r] = {'Data': p}

            # Fill in other useful values so you can use them without having the study handy
            for ptype in ['PairedBefore', 'PairedAfter', 'UnpairedBefore', 'UnpairedAfter']:
                p = s[ptype][r]
                p['DataType'] = ptype
                p['Study'] = s['Study']
                p['Radius'] = r
                p['Type'] = s['Type']

            # now compute the centroids and store as pandas.
            for ptype in ['PairedBefore', 'PairedAfter', 'UnpairedBefore', 'UnpairedAfter']:
                p = s[ptype][r]['Data']
                centroid = tuple([p['x'].mean(), p['y'].mean(), p['z'].mean()])
                pc = pd.DataFrame.from_records([centroid], columns=['x', 'y', 'z'])
                cname = ptype + 'Centroid'
                s[cname] = s.get(cname, dict())
                s[cname][r] = {'Data': pc}
                p = s[cname][r]
                p['DataType'] = cname
                p['Study'] = s['Study']
                p['Radius'] = r
                p['Type'] = s['Type']

            # Now compute the aligned images, if you have the tranformation matrix available.
            if s['Aligned']:
                image_obj = s['Alignment']
                s['AlignmentPts'][r] = {'Data': s['StudyAlignmentPts']}
                for ptype in ['PairedBefore', 'PairedAfter', 'UnpairedBefore', 'UnpairedAfter']:
                    p = pd.DataFrame(transform_points(image_obj.M_canonical, s[ptype][r]['Data'].loc[:, ['x', 'y', 'z']]),
                                     columns=['x', 'y', 'z'])
                    p['core'] = s[ptype][r]['Data']['core']

                    # Now do th aligned ....
                    datatype = 'Aligned' + ptype
                    s[datatype] = s.get(datatype, dict())
                    s[datatype][r] = {'Data': p}
                    s[datatype][r]['DataType'] = datatype
                    s[datatype][r]['Study'] = s['Study']
                    s[datatype][r]['Radius'] = r
                    s[datatype][r]['Type'] = s['Type']

                    # now compute the aligned centroids and store as pandas.
                    centroid = tuple([p['x'].mean(), p['y'].mean(), p['z'].mean()])
                    pc = pd.DataFrame.from_records([centroid], columns=['x', 'y', 'z'])
                    pc = pd.DataFrame.from_records([centroid], columns=['x', 'y', 'z'])
                    cname = datatype + 'Centroid'
                    s[cname] = s.get(cname, dict())
                    s[cname][r] = {'Data': pc}
                    s[cname]['DataType'] = cname
                    s[cname]['Study'] = s['Study']
                    s[cname]['Radius'] = r
                    s[cname]['Type'] = s['Type']
    return pairlist


def compute_studies(studyid, syn_pair_radii):
    """
    Compute the study pairs and build a python datastructure with all the pairs.  Result is a dictionary
    :param studyid: RID of the study cohort on which the pairs should be computed
    :param syn_pair_radii: tuple of radi over which pairs should be computed.
    :return:
    """
    studyset = get_studies(studyid)
    for k, v in group_studies(studyset['Studies'], group='Type').items():
        print('{0} {1}'.format(k, len(v)))
    studyset['Studies'] = compute_pairs(studyset['Studies'], syn_pair_radii)
    return studyset


def upload_studies(studyid, syn_pair_radii):
    """
    Compute the study pairs, dump out the python structure, upload to hatrac and link in as a data file associated
    with the study set.
    :param studyid: RID of the study cohort on which the pairs should be computed
    :param syn_pair_radii: tuple of radi over which pairs should be computed.
    :return:
    """
    studyset = compute_studies(studyid, syn_pair_radii)

    try:
        # Get a path for a temporary file to store  results
        tmpfile = os.path.join(tempfile.mkdtemp(), 'pairs-dump.pkl')
        with open(tmpfile, 'wb') as fo:
            pickle.dump(studyset, fo)
            print('dumped {0} studies to {1}'.format(len(studyset['Studies']), tmpfile))

        add_file_to_cohort(tmpfile, 'Python data structures with pair data for {0}'.format(studyid), studyid)
    finally:
        pass
  #     shutil.rmtree(os.path.dirname(tmpfile))
    return


def dump_studies(sset, fname):
    with open(fname, 'wb') as fo:
        pickle.dump(sset, fo)
        print('dumped {0} studies to {1}'.format(len(sset['Studies']), fname))


def fetch_studies(studyid):
    """
    Get the set of files associated with a cohort analysis.
    :param studyid: RID of the analysis cohort to which the file file should be assoicated.
    :return: None.
    """

    credential = get_credential(synapseserver)
    if '@' in studyid:
        catalog = ErmrestSnapshot('https', synapseserver, 1, credential)
    else:
        catalog = ErmrestCatalog('https', synapseserver, 1, credential).latest_snapshot()
    hatrac = HatracStore('https', synapseserver, credentials=credential)

    pb = catalog.getPathBuilder()
    zebrafish = pb.Zebrafish
    synapse = pb.Synapse

    # Lets get some shortcuts for awkward table names.
    cohort_table = zebrafish.tables['Cohort Analysis']
    collection = synapse.tables['Collection']

    path = cohort_table.alias('studyset').link(zebrafish.tables['Cohort Analysis_Collection']).link(collection)
    path = path.filter(path.studyset.RID == studyid)
    files = list(path.entities())

    if len(files) > 1:
        print('More then one data file, picking the first')

    try:
        # Get a path for a temporary file to store  results
        tmpfile = os.path.join(tempfile.mkdtemp(), 'pairs-dump.pkl')
        hatrac.get_obj(files[0]['URL'], destfilename=tmpfile)
        with open(tmpfile, 'rb') as fo:
            slist = pickle.load(fo)
    finally:
          shutil.rmtree(os.path.dirname(tmpfile))

    print('Restored {0} studies'.format(len(slist['Studies'])))
    return slist


def restore_studies(fname):
    """
    Restore a picked study set from a file.
    :param fname: name of file which contrains the pickled data.
    :return: the list of studies.
    """
    with open(fname, 'rb') as fo:
        slist = pickle.load(fo)

    print('Restored {0} studies'.format(len(slist['Studies'])))
    return slist


def copy_cohort(studyset, description=''):
    """
    Perform a deep copy of an analysis cohort, including links to associated image pairs and files
    :param studyset: RID of the source analysis cohort
    :param description: Text that is used to describe the new cohort
    :return: The RID of the new cohort.
    """

    # Need to use Deriva authentication agent before executing this

    credential = get_credential(synapseserver)
    if '@' in studyset:
        [studyset, version] = studyset.split('@')
        catalog = ErmrestSnapshot('https', synapseserver, 1, version, credentials=credential)
    else:
        catalog = ErmrestCatalog('https', synapseserver, 1, credentials=credential).latest_snapshot()

    systemcolumns = ['RMT', 'RID', 'RCT', 'RCB', 'RMB']

    pb = catalog.getPathBuilder()
    # convenient name for the schema and tables we care about.
    zebrafish = pb.Zebrafish
    cohort_table = zebrafish.tables['Cohort Analysis']
    pair_table = zebrafish.tables['Cohort Analysis_Synaptic Pair Study']
    collection_table = zebrafish.tables['Cohort Analysis_Collection']

    path = cohort_table.alias('studyset').link(pair_table)
    path = path.filter(path.studyset.RID == studyset)

    cohort = path.studyset.entities()[0]
    pairs = path.entities()
    files = path.studyset.link(collection_table).entities()

    # Now switch to the current catalog
    catalog = ErmrestCatalog('https', synapseserver, 1, credentials=credential)
    pb = catalog.getPathBuilder()
    zebrafish = pb.Zebrafish
    cohort_table = zebrafish.tables['Cohort Analysis']
    pair_table = zebrafish.tables['Cohort Analysis_Synaptic Pair Study']
    collection_table = zebrafish.tables['Cohort Analysis_Collection']

    newcohort = {}
    for i, v in cohort.items():
        if i not in systemcolumns:
            newcohort[i] = v
    newcohort['Description'] = \
        description if description != '' else 'Copy of {0}: {1}'.format(studyset, cohort['Description'])
    newcohortRID = cohort_table.insert([newcohort])[0]['RID']

    newpairs = []
    for p in pairs:
        newpair = {}
        for i, v in p.items():
            if i not in systemcolumns:
                newpair[i] = v
        newpair['Cohort Analysis'] = newcohortRID
        newpairs.append(newpair)
    pair_table.insert(newpairs)

    newfiles = []
    for f in files:
        newfile = {}
        for i, v in f.items():
            if i not in systemcolumns:
                newfile[i] = v
        newfile['Cohort Analysis'] = newcohortRID
        newfiles.append(newfile)
    collection_table.insert(newfiles)
    return newcohortRID


def add_file_to_cohort(file, description, cohort):
    """
    Upload a file into a data collection and add that file into the set of files associated with a cohort analysis.
    :param file: local path to the file that should be uploaded and associated with the cohort
    :param description: Text that is used to describe the file that is being uploaded
    :param cohort: RID of the analysis cohort to which the file file should be assoicated.
    :return: None.
    """
    credential = get_credential(synapseserver)
    store = HatracStore('https', synapseserver, credentials=credential)
    catalog = ErmrestCatalog('https', synapseserver, 1, credentials=credential)

    pb = catalog.getPathBuilder()
    zebrafish = pb.Zebrafish
    synapse = pb.Synapse

    collection = synapse.tables['Collection']
    files = collection.insert([{'Description': description}])
    newfileRID = files[0]['RID']

    path = '/hatrac/Data/Data_{0}_{1}'.format(newfileRID, os.path.basename(file))
    loc = store.put_obj(path, file)
    files[0]['URL'] = loc
    files[0]['Orig. Basename'] = os.path.basename(file)

    r = store.head(path)
    files[0]['MD5'] = r.headers['content-md5']
    files[0]['#Bytes'] = r.headers['Content-Length']
    files = collection.update(files)

    # Now link into cohort.
    collection_table = zebrafish.tables['Cohort Analysis_Collection']
    collection_table.insert([{'Cohort Analysis': cohort, 'Collection': newfileRID}])
    return